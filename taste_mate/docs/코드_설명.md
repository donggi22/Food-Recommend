# 코드 역할 & 동작 순서

## 동작 순서

```
context (더미) + candidates (더미 20개)
    → 룰 랭커 (ranker.py)
    → top_k (menu_id 리스트)
    → LLM (llm.py) → 1개 선택 + 사유 생성
    → JSON 반환
```

API 한 번에 쓰려면 **POST /v1/recommend** (context + candidates 넣으면 위 순서대로 끝까지 처리).

---

## 각 코드가 하는 일

| 파일 | 하는 일 |
|------|---------|
| **app/main.py** | FastAPI. `POST /v1/top-k` = 랭커만 (top_k만 반환). `POST /v1/recommend` = 랭커 → LLM → 추천+사유 JSON. `GET /`, `/v1/test-cases`, `/v1/candidates`는 프론트용. |
| **app/ranker.py** | 룰 랭커. context + candidates → 휴리스틱 점수 → 상위 K개 menu_id. |
| **app/llm.py** | 프롬프트 조립 → OpenAI 호출 → JSON 파싱. 실패 시 top_k[0] + fallback 문구. |
| **app/models.py** | Pydantic: Context, Candidate, RecommendRequest, ReasonResponse. |
| **app/logging_config.py** | recommend 호출 시 logs/reason_calls.jsonl에 기록. |
| **data/candidates.json** | 메뉴 20개 더미. |
| **data/test_cases.json** | 테스트용 context 10개. run_eval·프론트에서 사용. |
| **prompts/reason.txt** | LLM에 넣는 프롬프트 템플릿. {candidates_text} 자리에 후보 목록이 들어감. |
| **scripts/run_eval.py** | 테스트 케이스 10개로 API 호출 → 결과를 output/에 JSONL·CSV 저장, 검증(selected in top_k, 길이, context 키워드) 출력. |
| **scripts/run_reproducibility.py** | 같은 케이스 N번 호출해서 selected/reason 일치 여부 확인. |
